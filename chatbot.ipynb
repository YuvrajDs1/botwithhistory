{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73a2833",
   "metadata": {},
   "source": [
    "### Chatbot with Conversational History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dceb6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "585fb6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014A285151D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014A28515E50>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model='Gemma2-9b-It', groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a304336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You are Yuvraj, and you are a GenAI engineer!  \\n\\nIt seems I have a good memory for introductions. 😊  What can I help you with today?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 108, 'total_tokens': 147, 'completion_time': 0.070909091, 'prompt_time': 0.002811448, 'queue_time': 0.246072141, 'total_time': 0.073720539}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--1317288d-1b7e-4772-a366-c78e5a6af9d3-0', usage_metadata={'input_tokens': 108, 'output_tokens': 39, 'total_tokens': 147})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('user', 'Hi my name is Yuvraj and Im a GenAI engineer'),\n",
    "    (\"ai\", \"Hi Yuvraj, its nice to meet you. Thats fascinating, being a GenAI engineer is cutting edge and important work. What kind of projects are you working on? Im always eager to learn more about the advancements in the field.  Perhaps you could tell me about some of the challenges and rewards you encounter in your role?\"),\n",
    "    ('user', \"Whats my name and what do i do?\")\n",
    "])\n",
    "\n",
    "chain=prompt|model\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dccd2b",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use message history class to wrap our model and make it stateful. This will keep track of the inputs and outputs of the model and store them in some datastore. Future interactions will then load them and pass them into the chain as the input. Lets see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aed1b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4082b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={'configurable': {'session_id': 'chat1'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14626893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "response=with_message_history.invoke([\n",
    "    HumanMessage(content='Hi my name is Yuvraj and Im GenAI Engineer')],\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4cda4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Yuvraj, it's nice to meet you! \\n\\nBeing a GenAI Engineer is fascinating. What are you working on these days?  Are there any particular projects or areas of GenAI that you're particularly excited about?\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6288a4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Yuvraj and you are a GenAI Engineer. 😊  \\n\\nIs there anything else you'd like to know or talk about?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 94, 'total_tokens': 127, 'completion_time': 0.06, 'prompt_time': 0.002809718, 'queue_time': 0.253604841, 'total_time': 0.062809718}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--751a8f76-e37b-4621-815e-d00e550cc799-0', usage_metadata={'input_tokens': 94, 'output_tokens': 33, 'total_tokens': 127})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([\n",
    "    HumanMessage(content='Hi, what is my name and what do i do?')],\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30a7372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {'configurable': {'session_id': 'chat2'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54a30a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! 😊\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 15, 'total_tokens': 59, 'completion_time': 0.08, 'prompt_time': 0.001252549, 'queue_time': 0.24836557, 'total_time': 0.081252549}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f2442149-e5b1-49b3-ba70-16101177181c-0', usage_metadata={'input_tokens': 15, 'output_tokens': 44, 'total_tokens': 59})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changed session id using new config\n",
    "response=with_message_history.invoke([\n",
    "    HumanMessage(content='Hi what is my name?')],\n",
    "    config=config1)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cf878a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant. Answer all the questions to best of your ability'),\n",
    "    MessagesPlaceholder(variable_name='messages')\n",
    "])\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55333f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Yuvraj! It's nice to meet you.  \\n\\nWhat can I do for you today? 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 31, 'total_tokens': 60, 'completion_time': 0.052727273, 'prompt_time': 0.001481079, 'queue_time': 0.25645411, 'total_time': 0.054208352}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7a542b3c-a6ca-4141-a545-7da670e1a330-0', usage_metadata={'input_tokens': 31, 'output_tokens': 29, 'total_tokens': 60})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages': [HumanMessage(content='Hi my name is Yuvraj')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe1185b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history)\n",
    "config={'configurable': {'session_id': 'chat3'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38f26e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Yuvraj, it's nice to meet you!\\n\\nI'm happy to help with any questions you have. Just ask away! 😊  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 31, 'total_tokens': 66, 'completion_time': 0.063636364, 'prompt_time': 0.001493069, 'queue_time': 0.25770135, 'total_time': 0.065129433}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--57da9b00-67fc-4e23-8109-75a50201da35-0', usage_metadata={'input_tokens': 31, 'output_tokens': 35, 'total_tokens': 66})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke([\n",
    "    HumanMessage(content='Hi my name is yuvraj')],\n",
    "    config=config\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e52d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds more complexity\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant. Answer all the questions to the best of your ability in {language}'),\n",
    "    MessagesPlaceholder(variable_name='messages'),\n",
    "])\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3df60c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"नमस्ते Yuvraj! \\n\\nमुझे बहुत खुशी है कि आप मुझसे बात करना चाहते हैं। आपसे क्या जानना चाहेंगे? \\n\\n(Hello Yuvraj! I'm very happy you want to talk to me. What would you like to know?) \\n\\n\\nमुझे कोई भी प्रश्न पूछ सकते हैं, मैं अपनी पूरी कोशिश करूँगा कि इसका उत्तर दे सकूँ। \\n\\n(You can ask me any question, I will try my best to answer it.)\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 34, 'total_tokens': 146, 'completion_time': 0.203636364, 'prompt_time': 0.001487199, 'queue_time': 0.24810985, 'total_time': 0.205123563}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b0658672-48ab-44ba-a154-21c47a9966d9-0', usage_metadata={'input_tokens': 34, 'output_tokens': 112, 'total_tokens': 146})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({'messages': [HumanMessage(content='Hi My name is Yuvraj')], 'language': 'Hindi'})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b510418",
   "metadata": {},
   "source": [
    "Lets now wrap the more complicated chain in MessageHistory class. This time because there are multiple keys in the input we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab59998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='messages'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32188f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते Yuvraj! 👋 \\n\\nमुझे बहुत खुशी हो कि आप मुझसे बात कर रहे हैं।  \\n\\nआप क्या पूछना चाहेंगे? 😊 \\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={'configurable': {'session_id': 'chat4'}}\n",
    "response = with_message_history.invoke({'messages': [HumanMessage(content='Hi my name is Yuvraj')], 'language': 'Hindi'},\n",
    "                                       config=config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b944743",
   "metadata": {},
   "source": [
    "### Managing Chat conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63416681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like choclate ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Oh that great', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi again', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello again', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# trim_messages is a trimmer that allows to specify how many tokens we want to keep along with other parameters like if we want to always keep the system message and weather to allow partial messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=40, # defines the llms context window size\n",
    "    strategy='last',\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on='human'\n",
    "    \n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful assistant'),\n",
    "    HumanMessage(content='Hi how are you'),\n",
    "    AIMessage(content='Good'),\n",
    "    HumanMessage(content='Hi who are you'),\n",
    "    AIMessage(content='Im a AI assistant'),\n",
    "    HumanMessage(content='I like choclate ice cream'),\n",
    "    AIMessage(content='Oh that great'),\n",
    "    HumanMessage(content='Hi again'),\n",
    "    AIMessage(content='Hello again'),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fc75770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You like chocolate ice cream!  🍫  \\n\\nIs there a particular kind of chocolate ice cream you enjoy most?  😊 \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 73, 'total_tokens': 103, 'completion_time': 0.054545455, 'prompt_time': 0.002424558, 'queue_time': 0.257056192, 'total_time': 0.056970013}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--bc6d0f41-1bc7-4561-9d9c-dddb0606bc3d-0', usage_metadata={'input_tokens': 73, 'output_tokens': 30, 'total_tokens': 103})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter('messages')|trimmer)\n",
    "    |prompt\n",
    "    |model\n",
    ")\n",
    "\n",
    "response=chain.invoke({\n",
    "    'messages': messages + [HumanMessage(content='What ice cream do i like')],\n",
    "    'language':'English'}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e9ddcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap this in message history\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key='messages'\n",
    ")\n",
    "config={'configurable': {'session_id': 'chat5'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d219b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have personal preferences or memories, including your ice cream taste!  \\n\\nTo figure out what ice cream you like, think about:\\n\\n* **Your favorite flavors:** Chocolate, vanilla, strawberry, something more unique like pistachio or lavender?\\n* **Your preferred texture:** Creamy, chunky with mix-ins, sorbet, or something else?\\n* **Any toppings you love:** Sprinkles, nuts, whipped cream, fruit?\\n\\n\\nTell me about your favorite flavors and I can give you some ice cream suggestions! 🍨🍦  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 33, 'total_tokens': 152, 'completion_time': 0.216363636, 'prompt_time': 0.001481749, 'queue_time': 0.24829959, 'total_time': 0.217845385}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1c6c7d1-b965-424a-9bc9-65e84e26fc08-0', usage_metadata={'input_tokens': 33, 'output_tokens': 119, 'total_tokens': 152})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\n",
    "    'messages': [HumanMessage(content='What ice cream do i like')],\n",
    "    'language':'English'}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199884c",
   "metadata": {},
   "source": [
    "### Setting up vector stores and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "869ad607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content='Dogs are great companions',\n",
    "        meta_data={'source': 'mammal-pets-doc'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='Cats are independent pets',\n",
    "        meta_data={'source': 'mammal-pets-doc'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='Goldfish are popular',\n",
    "        meta_data={'source': 'fist-pets-doc'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='Parrots are intelligent birds',\n",
    "        meta_data={'source': 'bird-pets-doc'}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content='Rabbits are fast',\n",
    "        meta_data={'source': 'mammal-pets-doc'}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae9ce54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Dogs are great companions'),\n",
       " Document(metadata={}, page_content='Cats are independent pets'),\n",
       " Document(metadata={}, page_content='Goldfish are popular'),\n",
       " Document(metadata={}, page_content='Parrots are intelligent birds'),\n",
       " Document(metadata={}, page_content='Rabbits are fast')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fba3c809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014A282FB890>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014A22B77AD0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "os.environ['HUGGING_FACE_TOKEN']=os.getenv('HUGGING_FACE_TOKEN')\n",
    "\n",
    "llm=ChatGroq(model='llama-3.1-8b-instant', groq_api_key=groq_api_key)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de35a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d1984ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x14a28516490>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector stores\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6393d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='52f4df50-ceaa-4d29-9518-894c495d7b64', metadata={}, page_content='Cats are independent pets'),\n",
       " Document(id='94015632-ff01-44c1-8196-69fd418a52cf', metadata={}, page_content='Cats are independent pets'),\n",
       " Document(id='38e0a131-8b81-4bb4-a9d6-c0ff671131ef', metadata={}, page_content='Rabbits are fast'),\n",
       " Document(id='21a046fd-1b2d-4a98-b447-05f7570febfe', metadata={}, page_content='Rabbits are fast')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search('Cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca51776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='52f4df50-ceaa-4d29-9518-894c495d7b64', metadata={}, page_content='Cats are independent pets'),\n",
       " Document(id='94015632-ff01-44c1-8196-69fd418a52cf', metadata={}, page_content='Cats are independent pets'),\n",
       " Document(id='38e0a131-8b81-4bb4-a9d6-c0ff671131ef', metadata={}, page_content='Rabbits are fast'),\n",
       " Document(id='21a046fd-1b2d-4a98-b447-05f7570febfe', metadata={}, page_content='Rabbits are fast')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# async query\n",
    "await vectorstore.asimilarity_search('cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65faefe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='52f4df50-ceaa-4d29-9518-894c495d7b64', metadata={}, page_content='Cats are independent pets')],\n",
       " [Document(id='99016665-fc80-4771-88a8-f58087a49daf', metadata={}, page_content='Dogs are great companions')]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "retriever.batch(['cat', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29ec77d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='52f4df50-ceaa-4d29-9518-894c495d7b64', metadata={}, page_content='Cats are independent pets')],\n",
       " [Document(id='99016665-fc80-4771-88a8-f58087a49daf', metadata={}, page_content='Dogs are great companions')]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k':1}\n",
    ")\n",
    "retriever.batch(['cat', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54356a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here's what I can tell you about dogs:\\n\\nDogs are great companions.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message=\"\"\"\n",
    "Answer this question using the provuded context only. {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([('human',message)])\n",
    "\n",
    "rag_chain={'context': retriever, 'question': RunnablePassthrough()}|prompt|llm\n",
    "\n",
    "response=rag_chain.invoke('Tell me about dogs')\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80fe01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
